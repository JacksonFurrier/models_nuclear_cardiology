{"title":"Mathematical understanding of SPECT data","markdown":{"yaml":{"title":"Mathematical understanding of SPECT data"},"headingText":"Mathematical modelling of the \"acquisition\"","containsRefs":false,"markdown":"\n\nSingle-photon emission computed tomography (SPECT) data is inherently stochastic. It's stochastic nature comes from the nuclear decay of radioisotopes in the molecular structure of the radiopharmaceuticals.\n\nMany and most of the problems in SPECT imaging comes from the following formula of the inverse problem as an operator equation\n$$\nF(u) = g^{\\dagger},\n$$\nwhere the unknown quantity of interest is $u$, which can be described as an element in a real Banach-space $X$, the data $g^{\\dagger}$ is non-negative, integrable function on some compact manifold $\\mathbb{M} \\subset \\mathbb{R}^{d}$ and the possibly non-linear forward operator $F$ describes the imaging setup.\n\nIn general the ideal photon detection can be described by a Poisson point process (PPP) $G$, the density of which is the marginal spatial photon density $g^{\\dagger}$. Let $\\{x_{1}, \\dots, x_{N}\\} \\subset \\mathbb{M}$ denote the positions, where the photons were detected. The total number $N$ of detected photons is also random. Let $$G = \\sum_{i=1}^{N} \\delta_{x_{i}}$$ as a sum of Dirac-measures at the photon positions and denote by $$G(A)= \\#\\{1 \\leq i \\leq N | x_{i} \\in A \\}$$ the number of photons in a measurable subset $A \\subset \\mathbb{M}$. Then it is physically evident that $${\\bf E}[G(A)] = \\int_{A} g^{\\dagger} dx$$ and that the random variables $G(A_{i}), i\\in [1, M]$ for any finite number of disjoint, measurable sets $A_{i} \\subset \\mathbb{M}$ are stochastically independent. Hence, by definition, $G$ is a Poisson process, and as a consequence, $G(A)$ is a poisson distributed integer-valued random variable.\n\n## Poisson processes of SPECT data\nA point process on $\\mathbb{M}$ can be seen as a random collection of points $\\{ x_{i}, \\dots, x_{N} \\} \\subset \\mathbb{M}$ satisfying certain measurability properties. \n\n::: {.callout-note}\n## Definition (Poisson point process)\nLet $g^{\\dagger} \\in \\mathbf{L}^{1}(\\mathbb{M})$ with $g^{\\dagger} \\geq 0$. A point process $G = \\sum_{i=1}^{N} \\delta_{x_{i}}$ is called a Poisson point process (PPP) or Poisson process (PP) with intensity $g^{\\dagger}$ if\n\n(1) For each choice of disjoint, measurable sets $A_{1}, \\dots, A_{n} \\subset \\mathbb{M}$ random variables $G(A_{j})$ are stochastically independent. \n\n(2) ${\\bf E}[G(A)] = \\int_{A} g^{\\dagger} dx$ for each measurable set $A \\subset \\mathbb{M}$.\n:::\n\n::: {.callout-tip}\n## Proposition\nLet $G$ be a Poisson process with intensity $g^{\\dagger} \\in \\mathbf{L}^{1}(\\mathbb{M})$. Then for each measurable $A \\subset \\mathbb{M}$ the random variable $G(A)$ is Poisson distributed with parameter $\\lambda = \\int_{A} g^{\\dagger} dx$, i.e. ${\\bf P}[G(A) = k] = e^{-\\lambda} \\frac{\\lambda^{k}}{k!}$ for $k \\in \\mathbb{N}$.\n:::\n\n::: {.callout-tip}\n## Proposition\nPoisson process $G$ with intensity $g^{\\dagger} \\in \\mathbf{L}^{1}(\\mathbb{M})$ conditioned on $G(\\mathbb{M}) = N$ is a Bernoulli process with parameter $N$ and probability measure $\\mu(A) = \\int_{A} g^{\\dagger} / \\int_{\\mathbb{M}}g^{\\dagger}dx.$ In other words, conditioned on $G(\\mathbb{M}) = N$ the points are distributed like independent random variables $X_{1}, \\dots, X_{N}$ distributed according to $\\mu$.\n:::\n\n## Poisson process characteristics\nFew of the properties of the Poisson processes are the following\n\n::: {.callout-note}\n## Lawfulness\nIt means that the events are not arriving in the exact same time\n$$\\lim_{\\Delta t \\rightarrow 0} P(G(t + \\Delta t) - G(t) > 1\\ |\\ G(t + \\Delta t) - G(t) \\geq 1 ) = 0$$\n:::\n\n::: {.callout-note}\n## Memory-less, ageless\nThis means that the events arriving after another are stochastically independent. $$P(G(A) > a + b\\ |\\ G(A) > a) = P(G(A) > b) $$\n:::\n\nZleho ideas, Markov processes\n\n* Sensitivity is a constant multiplier on the pixel values\n\n* Pixel values are independent, isotropically independent, binning is independent\n\n* They are independent as much as phone calls, it is like histogramming the areas and centers\n\n* Gaussian approximation will be good with a weighted least squares, division by stdev will result in a good approximation\n\n* Photon decay, ket decay kozott exp az ido, linear transform of poisson will be our image\n\n* PDHG fix stepsize nem optimalis, lehetne rajta javitani\n\n## Projection data of SPECT\n\n\n```{python}\n#| echo : false\n# Loading the data\nimport plotly.graph_objects as go\nimport numpy as np\nimport nrrd\n\nnum_mph_proj = 16\nnum_par_proj = 16\n\nproj_data_mph, mph_header = nrrd.read('../data/class_webpage_projs_mph.nrrd')\nproj_data_par, par_header = nrrd.read('../data/class_webpage_projs_par.nrrd')\n```\n```{python}\n#| echo : false\nfrom IPython.display import HTML\nfrom numpy import random\nfrom matplotlib import animation\nimport matplotlib.pyplot as plt\n```\n```{python}\n#| title: Multi-pinhole projection frames with helical trajectory\n#| echo : false\n%%capture\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport matplotlib.animation as animation\n\nfig, ax = plt.subplots()\n\nims = []\nfor i in range(num_mph_proj):\n    im = ax.imshow(proj_data_mph[i], animated=True)\n    im.axes.get_xaxis().set_visible(False)\n    im.axes.get_yaxis().set_visible(False)\n    ims.append([im])\n\nani_mph = animation.ArtistAnimation(fig, ims, interval=50, blit=True,\n                                repeat_delay=1000)\n```\n```{python}\n#| echo : false\nHTML(ani_mph.to_jshtml())\n```\n\n```{python}\n#| title: Parallel (LEHR-HS) projection frames with helical trajectory\n#| echo : false\n%%capture\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport matplotlib.animation as animation\n\nfig, ax = plt.subplots()\n\nims = []\nfor i in range(num_par_proj):\n    im = ax.imshow(proj_data_par[i], animated=True)\n    im.axes.get_xaxis().set_visible(False)\n    im.axes.get_yaxis().set_visible(False)\n    ims.append([im])\n\nani_par = animation.ArtistAnimation(fig, ims, interval=50, blit=True,\n                                repeat_delay=1000)\n```\n\n```{python}\n#| echo : false\nHTML(ani_par.to_jshtml())\n```\n\n## Approximation in L2\n\n```{python}\n#| echo : false\n# Approximating the parallel geometries\nfrom scipy.optimize import least_squares\nimport numpy as np\n\ndef fun_poi(x):\n    return np.sum(np.linalg.norm(proj_data_par[0] - np.reshape(x, [64, 64])))\n\ndef fun_poi_std_dev(x):\n    std_dev = np.std(proj_data_par[0])\n    return np.sum(np.linalg.norm(proj_data_par[0] / std_dev - np.reshape(x, [64, 64])))\n\nx_0 = np.random.rand(64, 64) # start from a random matrix\n\nres = least_squares(fun_poi, x_0.flatten())\nres_std_dev = least_squares(fun_poi_std_dev, x_0.flatten())\n```\n\n\n```{python}\n#| echo : false\n#from sklearn.linear_model import LinearRegression\n#x = np.arange(0, 64 * 64, 1)\n#y = proj_data_par[0].flatten()\n\n#std_dev_proj = np.std(y)\n#w = np.where(y > 0, std_dev_proj, 1)\n\n#reg = LinearRegression().fit(x, y, w)\n\n```\n\n\nApproximating the original frames in $\\mathbb{L}_{2}$\n\n```{python}\n#| echo : false\nimport matplotlib.pyplot as plt\n\nappr_proj = np.reshape(res.x, [64, 64])\nappr_std_dev_proj = np.reshape(res_std_dev.x, [64, 64])\ndiff_orig_appr_proj = np.abs(appr_proj - proj_data_par[0])\n\nfig, axs = plt.subplots(1, 3)\n\naxs[0].imshow(appr_proj)\naxs[0].set_xlabel(\"Original frame\")\naxs[0].set_xticks([])\naxs[0].set_yticks([])\n\naxs[1].imshow(proj_data_par[0])\naxs[1].set_xlabel(\"Approx. frame\")\naxs[1].set_xticks([])\naxs[1].set_yticks([])\n\naxs[2].imshow(diff_orig_appr_proj)\naxs[2].set_xlabel(\"Difference frame\")\naxs[2].set_xticks([])\naxs[2].set_yticks([])\n\nprint(\"L2 approximation error (sum): \", np.sum(np.abs(appr_proj - proj_data_par[0])))\n\n```\n\n```{python}\nprof_orig_proj = proj_data_par[0, 32, :]\nprof_appr_proj = appr_proj[32, :]\nprof_appr_std_dev_proj = appr_std_dev_proj[32, :]\nprof_diff_proj = diff_orig_appr_proj[32, :]\n\nx = np.arange(0, 64, 1)\n\nfig, ax = plt.subplots()\n\nax.plot(x, prof_diff_proj)\n#ax.plot(x, prof_appr_std_dev_proj)\nax.set_xlabel(\"Profile curve of the difference\")\n\n```\n\n\n```{python}\nprint(np.std(proj_data_par[0]))\n```","srcMarkdownNoYaml":"\n\nSingle-photon emission computed tomography (SPECT) data is inherently stochastic. It's stochastic nature comes from the nuclear decay of radioisotopes in the molecular structure of the radiopharmaceuticals.\n\n## Mathematical modelling of the \"acquisition\"\nMany and most of the problems in SPECT imaging comes from the following formula of the inverse problem as an operator equation\n$$\nF(u) = g^{\\dagger},\n$$\nwhere the unknown quantity of interest is $u$, which can be described as an element in a real Banach-space $X$, the data $g^{\\dagger}$ is non-negative, integrable function on some compact manifold $\\mathbb{M} \\subset \\mathbb{R}^{d}$ and the possibly non-linear forward operator $F$ describes the imaging setup.\n\nIn general the ideal photon detection can be described by a Poisson point process (PPP) $G$, the density of which is the marginal spatial photon density $g^{\\dagger}$. Let $\\{x_{1}, \\dots, x_{N}\\} \\subset \\mathbb{M}$ denote the positions, where the photons were detected. The total number $N$ of detected photons is also random. Let $$G = \\sum_{i=1}^{N} \\delta_{x_{i}}$$ as a sum of Dirac-measures at the photon positions and denote by $$G(A)= \\#\\{1 \\leq i \\leq N | x_{i} \\in A \\}$$ the number of photons in a measurable subset $A \\subset \\mathbb{M}$. Then it is physically evident that $${\\bf E}[G(A)] = \\int_{A} g^{\\dagger} dx$$ and that the random variables $G(A_{i}), i\\in [1, M]$ for any finite number of disjoint, measurable sets $A_{i} \\subset \\mathbb{M}$ are stochastically independent. Hence, by definition, $G$ is a Poisson process, and as a consequence, $G(A)$ is a poisson distributed integer-valued random variable.\n\n## Poisson processes of SPECT data\nA point process on $\\mathbb{M}$ can be seen as a random collection of points $\\{ x_{i}, \\dots, x_{N} \\} \\subset \\mathbb{M}$ satisfying certain measurability properties. \n\n::: {.callout-note}\n## Definition (Poisson point process)\nLet $g^{\\dagger} \\in \\mathbf{L}^{1}(\\mathbb{M})$ with $g^{\\dagger} \\geq 0$. A point process $G = \\sum_{i=1}^{N} \\delta_{x_{i}}$ is called a Poisson point process (PPP) or Poisson process (PP) with intensity $g^{\\dagger}$ if\n\n(1) For each choice of disjoint, measurable sets $A_{1}, \\dots, A_{n} \\subset \\mathbb{M}$ random variables $G(A_{j})$ are stochastically independent. \n\n(2) ${\\bf E}[G(A)] = \\int_{A} g^{\\dagger} dx$ for each measurable set $A \\subset \\mathbb{M}$.\n:::\n\n::: {.callout-tip}\n## Proposition\nLet $G$ be a Poisson process with intensity $g^{\\dagger} \\in \\mathbf{L}^{1}(\\mathbb{M})$. Then for each measurable $A \\subset \\mathbb{M}$ the random variable $G(A)$ is Poisson distributed with parameter $\\lambda = \\int_{A} g^{\\dagger} dx$, i.e. ${\\bf P}[G(A) = k] = e^{-\\lambda} \\frac{\\lambda^{k}}{k!}$ for $k \\in \\mathbb{N}$.\n:::\n\n::: {.callout-tip}\n## Proposition\nPoisson process $G$ with intensity $g^{\\dagger} \\in \\mathbf{L}^{1}(\\mathbb{M})$ conditioned on $G(\\mathbb{M}) = N$ is a Bernoulli process with parameter $N$ and probability measure $\\mu(A) = \\int_{A} g^{\\dagger} / \\int_{\\mathbb{M}}g^{\\dagger}dx.$ In other words, conditioned on $G(\\mathbb{M}) = N$ the points are distributed like independent random variables $X_{1}, \\dots, X_{N}$ distributed according to $\\mu$.\n:::\n\n## Poisson process characteristics\nFew of the properties of the Poisson processes are the following\n\n::: {.callout-note}\n## Lawfulness\nIt means that the events are not arriving in the exact same time\n$$\\lim_{\\Delta t \\rightarrow 0} P(G(t + \\Delta t) - G(t) > 1\\ |\\ G(t + \\Delta t) - G(t) \\geq 1 ) = 0$$\n:::\n\n::: {.callout-note}\n## Memory-less, ageless\nThis means that the events arriving after another are stochastically independent. $$P(G(A) > a + b\\ |\\ G(A) > a) = P(G(A) > b) $$\n:::\n\nZleho ideas, Markov processes\n\n* Sensitivity is a constant multiplier on the pixel values\n\n* Pixel values are independent, isotropically independent, binning is independent\n\n* They are independent as much as phone calls, it is like histogramming the areas and centers\n\n* Gaussian approximation will be good with a weighted least squares, division by stdev will result in a good approximation\n\n* Photon decay, ket decay kozott exp az ido, linear transform of poisson will be our image\n\n* PDHG fix stepsize nem optimalis, lehetne rajta javitani\n\n## Projection data of SPECT\n\n\n```{python}\n#| echo : false\n# Loading the data\nimport plotly.graph_objects as go\nimport numpy as np\nimport nrrd\n\nnum_mph_proj = 16\nnum_par_proj = 16\n\nproj_data_mph, mph_header = nrrd.read('../data/class_webpage_projs_mph.nrrd')\nproj_data_par, par_header = nrrd.read('../data/class_webpage_projs_par.nrrd')\n```\n```{python}\n#| echo : false\nfrom IPython.display import HTML\nfrom numpy import random\nfrom matplotlib import animation\nimport matplotlib.pyplot as plt\n```\n```{python}\n#| title: Multi-pinhole projection frames with helical trajectory\n#| echo : false\n%%capture\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport matplotlib.animation as animation\n\nfig, ax = plt.subplots()\n\nims = []\nfor i in range(num_mph_proj):\n    im = ax.imshow(proj_data_mph[i], animated=True)\n    im.axes.get_xaxis().set_visible(False)\n    im.axes.get_yaxis().set_visible(False)\n    ims.append([im])\n\nani_mph = animation.ArtistAnimation(fig, ims, interval=50, blit=True,\n                                repeat_delay=1000)\n```\n```{python}\n#| echo : false\nHTML(ani_mph.to_jshtml())\n```\n\n```{python}\n#| title: Parallel (LEHR-HS) projection frames with helical trajectory\n#| echo : false\n%%capture\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport matplotlib.animation as animation\n\nfig, ax = plt.subplots()\n\nims = []\nfor i in range(num_par_proj):\n    im = ax.imshow(proj_data_par[i], animated=True)\n    im.axes.get_xaxis().set_visible(False)\n    im.axes.get_yaxis().set_visible(False)\n    ims.append([im])\n\nani_par = animation.ArtistAnimation(fig, ims, interval=50, blit=True,\n                                repeat_delay=1000)\n```\n\n```{python}\n#| echo : false\nHTML(ani_par.to_jshtml())\n```\n\n## Approximation in L2\n\n```{python}\n#| echo : false\n# Approximating the parallel geometries\nfrom scipy.optimize import least_squares\nimport numpy as np\n\ndef fun_poi(x):\n    return np.sum(np.linalg.norm(proj_data_par[0] - np.reshape(x, [64, 64])))\n\ndef fun_poi_std_dev(x):\n    std_dev = np.std(proj_data_par[0])\n    return np.sum(np.linalg.norm(proj_data_par[0] / std_dev - np.reshape(x, [64, 64])))\n\nx_0 = np.random.rand(64, 64) # start from a random matrix\n\nres = least_squares(fun_poi, x_0.flatten())\nres_std_dev = least_squares(fun_poi_std_dev, x_0.flatten())\n```\n\n\n```{python}\n#| echo : false\n#from sklearn.linear_model import LinearRegression\n#x = np.arange(0, 64 * 64, 1)\n#y = proj_data_par[0].flatten()\n\n#std_dev_proj = np.std(y)\n#w = np.where(y > 0, std_dev_proj, 1)\n\n#reg = LinearRegression().fit(x, y, w)\n\n```\n\n\nApproximating the original frames in $\\mathbb{L}_{2}$\n\n```{python}\n#| echo : false\nimport matplotlib.pyplot as plt\n\nappr_proj = np.reshape(res.x, [64, 64])\nappr_std_dev_proj = np.reshape(res_std_dev.x, [64, 64])\ndiff_orig_appr_proj = np.abs(appr_proj - proj_data_par[0])\n\nfig, axs = plt.subplots(1, 3)\n\naxs[0].imshow(appr_proj)\naxs[0].set_xlabel(\"Original frame\")\naxs[0].set_xticks([])\naxs[0].set_yticks([])\n\naxs[1].imshow(proj_data_par[0])\naxs[1].set_xlabel(\"Approx. frame\")\naxs[1].set_xticks([])\naxs[1].set_yticks([])\n\naxs[2].imshow(diff_orig_appr_proj)\naxs[2].set_xlabel(\"Difference frame\")\naxs[2].set_xticks([])\naxs[2].set_yticks([])\n\nprint(\"L2 approximation error (sum): \", np.sum(np.abs(appr_proj - proj_data_par[0])))\n\n```\n\n```{python}\nprof_orig_proj = proj_data_par[0, 32, :]\nprof_appr_proj = appr_proj[32, :]\nprof_appr_std_dev_proj = appr_std_dev_proj[32, :]\nprof_diff_proj = diff_orig_appr_proj[32, :]\n\nx = np.arange(0, 64, 1)\n\nfig, ax = plt.subplots()\n\nax.plot(x, prof_diff_proj)\n#ax.plot(x, prof_appr_std_dev_proj)\nax.set_xlabel(\"Profile curve of the difference\")\n\n```\n\n\n```{python}\nprint(np.std(proj_data_par[0]))\n```"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","output-file":"e_spect_data_math.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.6.39","title":"Mathematical understanding of SPECT data"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}