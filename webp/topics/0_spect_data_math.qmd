---
title: 1. SPECT Math
---

Single-photon emission computed tomography (SPECT) data is inherently stochastic. It's stochastic nature comes from the nuclear decay of radioisotopes in the molecular structure of the radiopharmaceuticals.

## Mathematical modelling of the "acquisition"
Many and most of the problems in SPECT imaging comes from the following formula of the inverse problem as an operator equation
$$
F(u) = g^{\dagger},
$$
where the unknown quantity of interest is $u$, which can be described as an element in a real Banach-space $X$, the data $g^{\dagger}$ is non-negative, integrable function on some compact manifold $\mathbb{M} \subset \mathbb{R}^{d}$ and the possibly non-linear forward operator $F$ describes the imaging setup.

In general the ideal photon detection can be described by a Poisson point process (PPP) $G$, the density of which is the marginal spatial photon density $g^{\dagger}$. Let $\{x_{1}, \dots, x_{N}\} \subset \mathbb{M}$ denote the positions, where the photons were detected. The total number $N$ of detected photons is also random. Let $$G = \sum_{i=1}^{N} \delta_{x_{i}}$$ as a sum of Dirac-measures at the photon positions and denote by $$G(A)= \#\{1 \leq i \leq N | x_{i} \in A \}$$ the number of photons in a measurable subset $A \subset \mathbb{M}$. Then it is physically evident that $${\bf E}[G(A)] = \int_{A} g^{\dagger} dx$$ and that the random variables $G(A_{i}), i\in [1, M]$ for any finite number of disjoint, measurable sets $A_{i} \subset \mathbb{M}$ are stochastically independent. Hence, by definition, $G$ is a Poisson process, and as a consequence, $G(A)$ is a poisson distributed integer-valued random variable.

## Poisson processes of SPECT data
A point process on $\mathbb{M}$ can be seen as a random collection of points $\{ x_{i}, \dots, x_{N} \} \subset \mathbb{M}$ satisfying certain measurability properties. 

::: {.callout-note}
## Definition (Poisson point process)
Let $g^{\dagger} \in \mathbf{L}^{1}(\mathbb{M})$ with $g^{\dagger} \geq 0$. A point process $G = \sum_{i=1}^{N} \delta_{x_{i}}$ is called a Poisson point process (PPP) or Poisson process (PP) with intensity $g^{\dagger}$ if

(1) For each choice of disjoint, measurable sets $A_{1}, \dots, A_{n} \subset \mathbb{M}$ random variables $G(A_{j})$ are stochastically independent. 

(2) ${\bf E}[G(A)] = \int_{A} g^{\dagger} dx$ for each measurable set $A \subset \mathbb{M}$.
:::

::: {.callout-tip}
## Proposition
Let $G$ be a Poisson process with intensity $g^{\dagger} \in \mathbf{L}^{1}(\mathbb{M})$. Then for each measurable $A \subset \mathbb{M}$ the random variable $G(A)$ is Poisson distributed with parameter $\lambda = \int_{A} g^{\dagger} dx$, i.e. ${\bf P}[G(A) = k] = e^{-\lambda} \frac{\lambda^{k}}{k!}$ for $k \in \mathbb{N}$.
:::

::: {.callout-tip}
## Proposition
Poisson process $G$ with intensity $g^{\dagger} \in \mathbf{L}^{1}(\mathbb{M})$ conditioned on $G(\mathbb{M}) = N$ is a Bernoulli process with parameter $N$ and probability measure $\mu(A) = \int_{A} g^{\dagger} / \int_{\mathbb{M}}g^{\dagger}dx.$ In other words, conditioned on $G(\mathbb{M}) = N$ the points are distributed like independent random variables $X_{1}, \dots, X_{N}$ distributed according to $\mu$.
:::

## Poisson process characteristics
Few of the properties of the Poisson processes are the following

::: {.callout-note}
## Lawfulness
It means that the events are not arriving in the exact same time
$$\lim_{\Delta t \rightarrow 0} P(G(t + \Delta t) - G(t) > 1\ |\ G(t + \Delta t) - G(t) \geq 1 ) = 0$$
:::

::: {.callout-note}
## Memory-less, ageless
This means that the events arriving after another are stochastically independent. $$P(G(A) > a + b\ |\ G(A) > a) = P(G(A) > b) $$
:::

Zleho ideas, Markov processes

* Sensitivity is a constant multiplier on the pixel values

* Pixel values are independent, isotropically independent, binning is independent

* They are independent as much as phone calls, it is like histogramming the areas and centers

* Gaussian approximation will be good with a weighted least squares, division by stdev will result in a good approximation

* Photon decay, between two decay time is exponential, linear transform of poisson will be our image

## Projection data of SPECT


```{python}
#| echo : false
# Loading the data
import plotly.graph_objects as go
import numpy as np
import nrrd

num_mph_proj = 16
num_par_proj = 16

proj_data_mph, mph_header = nrrd.read('../data/class_webpage_projs_mph.nrrd')
proj_data_par, par_header = nrrd.read('../data/class_webpage_projs_par.nrrd')
```
```{python}
#| echo : false
from IPython.display import HTML
from numpy import random
from matplotlib import animation
import matplotlib.pyplot as plt
```
```{python}
#| title: Multi-pinhole projection frames with helical trajectory
#| echo : false
%%capture


import matplotlib.pyplot as plt
import numpy as np
import matplotlib.animation as animation

fig, ax = plt.subplots()

ims = []
for i in range(num_mph_proj):
    im = ax.imshow(proj_data_mph[i], animated=True)
    im.axes.get_xaxis().set_visible(False)
    im.axes.get_yaxis().set_visible(False)
    ims.append([im])

ani_mph = animation.ArtistAnimation(fig, ims, interval=50, blit=True,
                                repeat_delay=1000)
```
```{python}
#| echo : false
HTML(ani_mph.to_jshtml())
```

```{python}
#| title: Parallel (LEHR-HS) projection frames with helical trajectory
#| echo : false
%%capture


import matplotlib.pyplot as plt
import numpy as np
import matplotlib.animation as animation

fig, ax = plt.subplots()

ims = []
for i in range(num_par_proj):
    im = ax.imshow(proj_data_par[i], animated=True)
    im.axes.get_xaxis().set_visible(False)
    im.axes.get_yaxis().set_visible(False)
    ims.append([im])

ani_par = animation.ArtistAnimation(fig, ims, interval=50, blit=True,
                                repeat_delay=1000)
```

```{python}
#| echo : false
HTML(ani_par.to_jshtml())
```

## Approximation in L2

```{python}
#| echo : false
# Approximating the parallel geometries
from scipy.optimize import least_squares
import numpy as np

def fun_poi(x):
    return np.sum(np.linalg.norm(proj_data_par[0] - np.reshape(x, [64, 64])))

def fun_poi_std_dev(x):
    std_dev = np.std(proj_data_par[0])
    return np.sum(np.linalg.norm(proj_data_par[0] / std_dev - np.reshape(x, [64, 64])))

x_0 = np.random.rand(64, 64) # start from a random matrix

res = least_squares(fun_poi, x_0.flatten())
res_std_dev = least_squares(fun_poi_std_dev, x_0.flatten())
```


```{python}
#| echo : false
#from sklearn.linear_model import LinearRegression
#x = np.arange(0, 64 * 64, 1)
#y = proj_data_par[0].flatten()

#std_dev_proj = np.std(y)
#w = np.where(y > 0, std_dev_proj, 1)

#reg = LinearRegression().fit(x, y, w)

```


Approximating the original frames in $\mathbb{L}_{2}$

```{python}
#| echo : false
import matplotlib.pyplot as plt

appr_proj = np.reshape(res.x, [64, 64])
appr_std_dev_proj = np.reshape(res_std_dev.x, [64, 64])
diff_orig_appr_proj = np.abs(appr_proj - proj_data_par[0])

fig, axs = plt.subplots(1, 3)

axs[0].imshow(appr_proj)
axs[0].set_xlabel("Original frame")
axs[0].set_xticks([])
axs[0].set_yticks([])

axs[1].imshow(proj_data_par[0])
axs[1].set_xlabel("Approx. frame")
axs[1].set_xticks([])
axs[1].set_yticks([])

axs[2].imshow(diff_orig_appr_proj)
axs[2].set_xlabel("Difference frame")
axs[2].set_xticks([])
axs[2].set_yticks([])

print("L2 approximation error (sum): ", np.sum(np.abs(appr_proj - proj_data_par[0])))

```

```{python}
prof_orig_proj = proj_data_par[0, 32, :]
prof_appr_proj = appr_proj[32, :]
prof_appr_std_dev_proj = appr_std_dev_proj[32, :]
prof_diff_proj = diff_orig_appr_proj[32, :]

x = np.arange(0, 64, 1)

fig, ax = plt.subplots()

ax.plot(x, prof_diff_proj)
#ax.plot(x, prof_appr_std_dev_proj)
ax.set_xlabel("Profile curve of the difference")

```


```{python}
print(np.std(proj_data_par[0]))
```